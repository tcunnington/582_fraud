\documentclass{article}

\input{MathTopMatter.tex}

\usepackage{fullpage}

\usepackage[titletoc,title]{appendix}

%\renewcommand{\lstlistingname}{Code}

\title{AMATH 582: Final Project}
\date{March \nth{16}, 2017}
\author{Lee Burke \\ Taylor Cunnington}

\renewcommand{\thesection}{Sec.~\Roman{section}}
%\renewcommand{\thesubsection}{\arabic{section} (\alph{subsection})}
%\renewcommand{\thesubsubsection}{\arabic{section} (\alph{subsection}) (\roman{subsubsection})}

\usepackage{chngcntr}
\counterwithout{figure}{section}

\begin{document}
\maketitle
\abstract{
 We consider the challenges in working with highly imbalanced data--data where the number of observations in class A far outnumber the number of observations in class B. The dataset we use for this project is a perfect example: credit card data from european cardholders where some transactions are fraudulent. To a credit card company instances of fraud are relatively rare in comparison to all transactions, but they are particularly important to be able to identify. The challenge with this type of dataset is twofold: 1. classification algorithms do not do a good job at making predictions when trained on such data, and 2. the common classifier performance metrics do not work to identify models that specifically predict the minority class well. This latter point is important because typically if you are considering a minority class then it is particularly important for prediction. Our central topic of investigation is how to evaluate classification techniques in a way that captures the importance of classifying the minority class correctly. To do so we attempt multiple classification techniques, as well as some specialized methods to deal specifically with this problem.

}
\section{Introduction and Overview}
Classifying bank transactions as fraudulent is a difficult task: (1) data is highly sensitive and so difficult to come by, (2) relevant features are not always clear, and (3) classification algorithms have trouble with unbalanced data where fraud is much less common than honest purchases.

To answer the first problem, we analyze real credit card data from a challenge on \href{kaggle.com}{Kaggle.com}. However, to preserve confidentiality, the features have been obscured by a singular value decomposition (SVD): we are given $U\Sigma$ if the original data $A=U\Sigma V^\ast$ in the usual SVD. That is, we have no say in the second problem, feature extraction. Indeed, the PCA modes have no discernable structure, and decay slowly. Instead, we focus on classifying skewed data.

That popular algorithms have trouble with skew is, in some sense, expected: minimizing the error rate inherently favors the major class. Guessing the larger class every time is already above $50\%$ accuracy. In addition, overall error is not the metric we care most about: false positives (mistakenly flagging a transaction as fraud) have much lower cost to the bank and consumer than false negatives (someone gets away with fraud). Our dataset is a great example of imbalanced data. Only $0.17\%$ of the data points are classified positive. It is difficult to improve on $99.83\%$ accuracy!

We apply several methods learned in lecture, including linear discriminant analysis (LDA), a binary classification tree (BCT), and a support vector machine (SVM). A new method is used to reduce the skew in the class sizes in combination with LDA. We focus especially on how to compare the performance of these classifiers, because common performance metrics may not be as descriptive with high-cost, highly-skewed data.

\section{Theoretical Background}
A brief theoretical exploration of each classification scheme and its evaluation with respect to imbalanced data are given below.

\subsection{Classification}
\subsubsection{Linear Discriminant Analysis}
The LDA algorithm, \texttt{fitcdiscr} in MATLAB, builds normal probability distributions for each class with mean and covariance parameters $\left({\vec {\mu }}_{0}, \sigma_{0} \right) \left({\vec {\mu }}_{0}, \sigma_{0}\right) and \left({\vec {\mu }}_{1},\sigma_{1}\right) \left({\vec {\mu }}_{1}, \sigma_{1}\right)$
These are referred to as the prior distributions. Classifying subsequent observations is then potentially just a matter of comparing their probability of being in class 0 or 1 and applying a threshold.
MATLAB actually attempts to minimize the cost function:
\begin{equation}
\hat{y}=\arg\min_{y=1,...,K}\sum_{k=1}^{K}\hat{P}(k|x)C(y|k)
\end{equation}
where $\hat{y}$ is the predicted probability. $K$ is the number of classes. $\hat{P}$ is the posterior probability of class $k$ for a given observation $x$, and $C(y|k)$ is the cost of misclassifying $k$ as $y$.
$\hat{P}$ is determined from Bayes rule:
\begin{equation}
\hat{P}(k|x)=\frac{P(x|k)P(k)}{P(x)}
\end{equation}
Here the posterior probability that $x$ is in class $k$ is what LDA estimates from the training data.
$P(k)$ and $P(x)$ are readily calculated the from the data.
Note that for the case of an extremely small minority class such as ours $P(n)$ will be extremely small (where $k=n$); this means that $\hat{P}(n|x)$ will be small as well.
This is a source of bias for LDA toward the majority class.

\subsubsection{Binary Classification Trees}

Binary decision trees (BCT) are a common and effective choice for classification tasks.
The MATLAB command to build a BCT model, \texttt{fitctree}, uses an algorithm called CART, for Classification and Regression Tree. You classify an unknown observation by starting at the root node and at each node in the tree you make binary decision (meaning there are two branches) based on the features of the data used for training. You work your way up the tree until you hit a terminal leaf node. This node determines your prediction.

You can think about this process as splitting up m-dimensional space into \say{blocks} (if you imagine the 3-dimensional version), where m is the number of features and each block represents a class prediction. It starts with the full space and splits it into two, then one of those subspaces in two, and then one of those three subspaces into two, and onwards until it finishes.
The algorithm chooses where to put the boundaries based on minimizing the Gigi index:
\begin{equation}
G={\textstyle \sum_{k}p_{k}(1-p_{k})}
\end{equation}
Here $p_k$ is the proportion of points in a subsection of class $k$, summed over all classes.
Note that the minimum you can achieve is 0 for a pure class and $0.50$ for half of each class. It calculates the Gini index for all the potential split points and chooses the lowest one in a greedy fashion--it does not consider global error. The criterion for no longer continuing to split the subspaces is often that you have too few training instances at a node.

In the case of imbalanced data $p_k$ will always be very small for the minority class in the general case where the classes are highly overlapping. Even if the algorithm has narrowed down to a box that just barely contains all the minority class observations and just a small proportion of the majority class observations, it is still very possible that the number of majority class points in that space will be of a similar number or greater than the minority class. The Gini index naturally gives high weight to the more represented class, which is a bias for the majority class.

\subsubsection{Support Vector Machines}
SVM attempts to find an $m$-dimensional hyperplane separating two classes, where $m$ is the number of features.
In the general case two classes are not linearly separable and so the algorithm attempts to minimize the cost function,
\begin{equation}
	\left[\frac{1}{n}\sum_{i=1}^{n}\max(0,1-y_{i}(\overrightarrow{w}\centerdot\overrightarrow{x_{i}}-b))\right]
\end{equation}
where $\overrightarrow{x_{i}}$ and $y_{i}$ are a training observation and the assigned class, and $\vec{w} \cdot {\vec {x}}-b=0$ is the equation of a hyperplane. Essentially what this gives is 0 for data points on the correct side of the plane and a positive value for points on the incorrect side, with magnitude proportional to the distance from the plane.

Like the above two schemes it treats points from each class the same. It does not treat the minority class as more important to classify correctly, and since there are many more observations from the majority class this method will also be biased towards the majority class.

\subsection{Handling Class Skew}
There are some known methods for improving performance when classifying imbalanced data. The four common solution types are given here:
\begin{enumerate}
\item Under-sampling -- Removing data from the larger class to balance the class sizes.
May lead to a poorer choice of decision line due to losing data at the border of the classes.
\item Oversampling -- Adding extra observations on top of existing minority class observations to balance out the class sizes.
May lead to overfitting with some classification models.
\item Synthetic data generation -- Generating artificial data from your existing data to balance classes.
Generated data generally stays within the n-dimensional volume that minimally encloses the existing data.
\item Cost functions-- Classification algorithms use cost functions (decision functions) to define their decision boundaries.
With imbalanced data you would set the misclassification of the minority class to be much more costly, to encourage the algorithm to classify them correctly more often than the majority class.
\end{enumerate}
The first three attempt to reduce the imbalance by reducing the number of majority data point or increasing the number of minority data points.
These methods deal with the data only, not the classifier.
The final option does alter the classifier however, to try to make it classify the minority class more reliably.

\subsection{Performance Evaluation}
The first metric for evaluating a classification scheme is the \emph{confusion matrix}. We write \say{true} and \say{false} to mean correct and incorrect classifications, and in a binary classification like in this paper, we write \say{positive} and \say{negative} for the two class labels. Then we can construct a matrix which counts the total number of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) labels:
\begin{align}
	C = \bcm TP & FN \\ FP & TN \ecm.
\end{align}
While this is a convenient, compact measure of performance, it does not tell the whole story. Most prominently, accuracy is easily defined as $(TP+TN)/(TP+TN+FP+FN)$, the number of correct classifications divided by the total number of classifications. This kind of \emph{rate} is best for describing behavior regardless of test sample size.

\subsubsection{Receiver Operating Characteristics (ROC) Curve}
It is often useful to consider the columns of the confusion matrix $C$ separately: this decouples discussions of accuracy from class skew and cost considerations. In particular, for classifiers with continuous output (of which all of the algorithms in Section II.1 are capable) the receiver operating characteristics (ROC) curve plots the true positive rate ($TPR$) against the false positive rate ($FPR$) for all threshold values.

For test sets with a finite number of samples, this curve will be discontinuous (as the threshold passes each sample). The curve begins in the bottom left corner (all samples are labeled negative, so $TP=0$ and $FP=0$) and ends in the top right corner (all samples are labeled positive, so $TPR=1$ and $FPR=1$). The threshold value is chosen at the point \say{closest} to the top left corner ($TPR=1$ and $FPR=0$). Examining ROC curves visually, we can inspect how various classification algorithms approach this optimal condition, and what that says about how their possible behavior on future test sets.

\subsubsection{Precision/Recall (PR) Curve}
Another common metric is the Precision/Recall (PR) curve. Precision is defined as the number of true positive predictions divided by the total number of positive predictions: it is how frequently positive predictions are correct. Recall is defined as the number of true positive predictions divided by the total true positive samples in the test set: it is how frequently true positive samples were correctly classified. PR curves plot precision against recall for all threshold values of a continuous-output classifier.

In contrast to ROC curves, PR curves are sensitive to class skew: they start near the top left corner (all samples are labeled negative so precision is not defined, but it is likely that nearly all will be correct for low threshold values) then end at the ratio of true positive to true negative samples (all samples are labeled positive, so recall is perfect while precision is exactly that ratio). \say{Better} schemes will approach the top right corner (where positive predictions are usually correct, and most of the true positive samples are correctly identified).

Clearly, the PR curve is a better tool than the ROC curve for the application considered in this project: false positives can be tolerated, so long as most of the true positives are correctly classified (recall) and most of the positive hits actually are fraud (precision).

\subsubsection{Area Under the Curve}
We often want to reduce subjective, holistic comparisons of these curves into single values. One common way to do this is to compute the area under the curve (AUC for ROC curves and AUCPR for PR curves). This provides a metric for how \say{quickly} the classification scheme can approach ideal classification---in a sense, how robust it is to the optimal choice of threshold. For ROC curves, this is well defined: all ROC curves begin in the lower left corner and end in the upper right corner. Indeed, AUC can be shown to be the likelihood that a positive sample is ranked higher than a negative sample\cite{Fawcett}.

PR curves do not have this property: they vary wildly in how they approach the left boundary, and if the threshold values are not chosen fine enough, it may not even start near the left boundary. However, we include discussion of this value below. It can be shown that AUCPR is the fraction of positive samples whose values exceed a random threshold\cite{boyd}.

Many methods exist to compute both of these quantities, and confidence intervals on the curves\cite{boyd}\cite{fawcett}. In this project, we use only the built-in methods from Matlab's \texttt{perfcurve}, and do not perform bootstrapping or other methods to find confidence intervals.

\section{Algorithm Implementation and Development}

As stated above our data was provided pre-prepared and instead of human understandable features it contained SVD modes, thus we did not alter or clean the data.
We created a \say{full} data matrix $X$ from the 28 columns of U and the amount of the transaction.
We used standard Z-score of the $X$ matrix, $Z$, for all implementations of the LDA classifier.
Then we randomly split the data to conduct a Monte-Carlo cross validation, using 80\% of each class for training and 20\% for testing.

In order to compare methods we chose three classification schemes: LDA, classification trees, and SVM. LDA was our primary classification algorithm. We used it to investigate the synthetic data generation and alternate cost function approaches by comparing to the default algorithm.
For synthetic data generation we chose to use ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning) a generation algorithm similar to the well known SMOTE (Synthetic Minority Over-Sampling Technique) algorithm, which is ADASYN's precursor.
The goal of both is to improve the class balance by increasing the number of minority class members.
SMOTE places synthetic data between existing data points randomly (linear interpolation), with no preference shown to any specific points.
ADASYN does the same thing but places more synthetic data points close to the boundary between classes because those are the original data points that are more difficult to learn.

We also tried changing the cost function of our LDA model to discourage FNs.
Since LDA works by creating probability distributions the cost comes into play only when making predictions.
Each observation that it is trying to predict has a calculated posterior probability.
It tries to minimize the "classification cost"--this is a decision cost function.
Unfortunately if an observation has an extremely small posterior probability then even with a vary large cost for miscalculating that observation it may not change the classification cost by much, meaning there will be little change to the resulting prediction.

The SVM and tree classifiers were not altered. They were included to allow us to compare some ROC and/or PR curves for non-LDA approaches.

\section{Computational Results}

The confusion matrices for each classification scheme are:
\begin{align}
	\text{LDA:} & \bcm
       56855    &       8 \\
          30     &     69
          \ecm \\
	\text{LDA w/ADASYN:} & \bcm
              56162       &  701 \\
          16       &   83
          \ecm \\
	\text{BCT:} & \bcm
              56845  &        18 \\
          31       &   68
          \ecm \\
	\text{SVM:} & \bcm
       56851 &         12 \\
          29      &    70
          \ecm
\end{align}
We see the LDA with resampling outperforming the others in true positive classification, though it has orders of magnitude more false positives. Indeed, Figure 1 shows the ROC curves for each classification scheme. LDA and LDA with resampling are close contenders for most accurate, with SVM lagging far behind, and BCT skipping directly from \say{all negative} to \say{all positive} (this is likely a byproduct of Matlab's \say{continuous} output from BCT). It seems the resampling lets LDA outperform (in true positive rate) the otherwise competitive schemes, at a high cost of false positives.
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{ROC}
	\caption{The ROC curve for LDA (blue), LDA with resampling (red), BCT (yellow), and SVM (purple) is shown. The LDA curves outperform the others here.}
	\label{fig:roc}
\end{figure}

Figure 2 shows the PR curves for the classification schemes. LDA still outperforms the other methods, though SVM is much closer to the front of the pack: and indeed, manages cover more of the recall domain than LDA. LDA with resampling has degraded to one of the worst schemes: positive predictions are not usually correct for this scheme). Nonetheless, both SVM and LDA are able to maintain precision above $80\%$ for recall above $70\%$, though better performance than this eludes these black-box algorithms.
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{PR}
	\caption{The PR curve for LDA (blue), LDA with resampling (red), BCT (yellow), and SVM (purple) is shown. The LDA without resampling outperforms the others, with admirable performance by the SVM.}
	\label{fig:pr}
\end{figure}

The area under these curves is tabulated in Table 1. The AUC corroborates the results above: LDA with resampling narrowly outperforms simple LDA, with SVM and BCT trailing. However, the AUCPR has some spurious results due to the threshold values chosen not producing the full curve. It seems likely that LDA would stay above $90\%$ for the other $0.7$ recall values, resulting in an additional area of about $0.65$, allowing it to outstrip the other methods.
\begin{table}[!htb]
	\centering
	\caption{The area under the ROC and PR curves are summarized for each of the classification schemes discussed above.}
	\label{tab:area}
	\begin{tabular}{r|c|c}
		& AUC & AUCPR \\
		\hline
		LDA & 0.976 & 0.124 \\
		LDA w/ADASYN & 0.980 & 0.538 \\
		BCT  & 0.868 & 0.416 \\
		SVM & 0.905 & 0.660
	\end{tabular}
\end{table}

\section{Summary and Conclusions}
Fraud detection is a difficult classification problem for three reasons: (1) data procurement, (2) feature extraction, and (3) class skew. This project investigates the latter problem exclusively, applying four classification algorithms, and analyzing the results with ROC curves and PR curves, and the area under those curves.

We find that simple (and fast) LDA performs as well or better than the other schemes in both an ROC and PR context. Resampling using the ADASYN package allows for a higher true positive rate at the cost of many more false positives. Defining a cost function (an engineering or business question  more than a mathematical one) could help find a balance in this trade off. No method tested is able to find complete separation of the classes.

More sophisticated tools may be able to achieve this separation. The use of nonlinear schemes and more careful application of sampling may allow for higher recall without the burden of higher false positive rates. Furthermore, more careful validation of the methods in this study is needed: there are several methods available for this process (including bootstrapping, $k$-fold cross-validation, and others\cite{boyd}) which can provide reasonable confidence in the ability of these methods to predict results for \say{wild} data.

\clearpage
\begin{thebibliography}{9}

\bibitem{boyd}
Boyd, K., Eng, K. H., \& Page, C. D. (2013). Area under the Precision-Recall Curve: Point Estimates and Confidence Intervals. \emph{Machine Learning and Knowledge Discovery in Databases Lecture Notes in Computer Science}, 451-466.

\bibitem{fawcett}
Fawcett, Tom (2006). "An Introduction to ROC Analysis" (PDF). \emph{Pattern Recognition Letters. 27} (8): 861â€“874

\bibitem{pozzolo}
Pozzolo, A. D., Caelen, O., Johnson, R. A., \& Bontempi, G. (2015). Calibrating Probability with Undersampling for Unbalanced Classification. \emph{2015 IEEE Symposium Series on Computational Intelligence}.

\end{thebibliography}

\begin{appendices}
\section{MATLAB functions used and brief implementation explanation}
\begin{itemize}
	\item \texttt{fitctree} -- Binary classification tree
	\item \texttt{fitcsvm} -- SVM classifier
	\item \texttt{fitcdiscr} -- Linear discriminant analysis
	\item \texttt{ADASYN} -- Synthetic data generation via the ADASYN algorithm
	\item \texttt{predict}
	\item \texttt{perfcurve}
\end{itemize}

\section{MATLAB codes}
\lstinputlisting[caption=Main, label=code:main]{../main.m}
\lstinputlisting[caption=Split data into training and test sets, label=code:splitBinaryClassData]{../splitBinaryClassData.m}
\lstinputlisting[caption=Random permutation for indices, label=code:splitIndices]{../splitIndices.m}
\lstinputlisting[caption=Performance characteristics of a classification model, label=code:evaluate]{../evaluate.m}
\end{appendices}
\end{document}
